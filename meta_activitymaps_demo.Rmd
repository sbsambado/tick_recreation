---
title: "demo"
output: html_document
date: "2025-09-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## upload necessary packages
library(sf)
library(terra)
library(tidyverse)
library(tigris)
library(data.table)
library(leaflet)
library(RColorBrewer)
library(slippymath)

## upload shapefile -- make this whatever polygon you want to extract to could be county or census tract
# california state parks shp (casp)
casp_shp <- st_read("data/raw/parks/casp_individualpark_boundaries/ParkBoundaries.shp")

```

#Meta Activity Maps
- I only use visit_* tiles (not home_* tiles)
- I only use daytime observations (not nighttime)


##Step 1. Reduce memory burden
- the US csv file was quite large so I had to fiddle around with filtering/making sure my computer didn't explode (play around with this)
```{r}

## set up params
chunk_size <- 500000 # number of lines to read in per chunk (helps with memory)
input_file <- "data/raw/recreation/meta/activitymaps/activity_space_distributions_20250908_us1.csv"
output_file <- "data/processed/meta_activitymaps_us1.csv"

# filter for california (currently this is all of US)
# looked at casp_shp_bbox and added + 0.2
lat_min <- 32.3 
lat_max <- 42.1
lon_min <- -125 # western boundary
lon_max <- -114 # eastern boundary

# choose zoom level (larger zoom values = smaller tiles)
zoom <- 13 # may fiddle with this (10 is way too coarse for me)

## deleting exisiting output file (helps memory)
# initial output - makes we we dont append to old runs results
if(file.exists(output_file)) file.remove(output_file)

## read and filter csv into chunks
con <- file(input_file, "r") # open connection to large csv
header <- readLines(con, n = 1) # read first line w/header

## process the csv in chunks (helps memory)
while(length(chunk_lines <- readLines(con, n = chunk_size)) > 0 ) {
  
  # combine header with current chunk lines for data table
  chunk <- fread(paste(c(header, chunk_lines), collapse = "\n"))
  
  # filter chunk for rows that fall within CA boundary
  chunk[
    visit_latitude >= lat_min & visit_latitude <= lat_max &
      visit_longitude >= lon_min & visit_longitude <= lon_max
  ][
    , fwrite(.SD, output_file, append = TRUE) # write only filtered data
  ]
}

close(con) # close file connection

```

##Step 2. load in filtered data & wrangle it
```{r}

# load filtered data into memory (vs load in full lg csv)
ca_data <- fread(output_file) 

## summaries daytime activity
activity_summary <- ca_data %>% 
  filter(day_or_night == "daytime") %>% # may switch to night for heat-ag proj
  group_by(visit_latitude, visit_longitude) %>% # only interested in visit location not home
  summarise(activity = max(visit_fraction, na.rm = TRUE), .groups = "drop") # could use mean

## compute activity indices for each tile location
tile_df <- activity_summary %>% 
  rowwise() %>% # need for lonlat_* since its returned as a list per row
  mutate(tile = list(lonlat_to_tilenum(visit_longitude, visit_latitude, zoom))) %>% 
  mutate(xtile = tile$x, ytile = tile$y) %>% # extract x & y index of tile
  ungroup()

## aggregate activity by tile 
# remember i'm using max here but could be mean
tile_activity <- tile_df %>% 
  group_by(xtile, ytile) %>% 
  summarise(activity = max(activity), .groups = "drop")

## compute bounding box for each tile
# for each tile (xtile, ytile) comput its bbox in lat/lon coords
bbox_df <- tile_activity %>% 
  rowwise() %>% 
  mutate(bbox = list(tile_bbox(xtile, ytile, zoom))) %>% 
  mutate(lng1 = bbox$xmin, lng2 = bbox$xmax, # left/right longitudes
         lat1 = bbox$ymin, lat2 = bbox$ymax) %>%  # bottom/top latitudes
  ungroup()

## create tile polygons
# build each tile polygon using its 4 corners, and close it by repeating the first point
polygon_list <- purrr::pmap(list(bbox_df$lng1, bbox_df$lng2, bbox_df$lat1, bbox_df$lat2),
                            function(xmin, xmax, ymin, ymax) {
                              st_polygon(list(matrix(c(
                                 xmin, ymin, # bottom left
                                 xmax, ymin, # bottom eirght
                                 xmax, ymax, # top right
                                 xmin, ymax, # top left
                                 xmin, ymin), # close polygon
                                 ncol = 2, byrow = TRUE)))
                            })

## create sf object
# attach activity values & geometry into a spatial object
tile_activity_sf <- st_sf(activity = bbox_df$activity,
                          geometry = st_sfc(polygon_list, crs = 3857)) # check, but i thik tile_bbox gives web mercator coords

# convert 3857 to 4326 for leaflet & downstream analysis
tile_activity_lonlat <- st_transform(tile_activity_sf, crs = 4326) 
## ^^^ this is the dataset used to summarize later
```

##Step 3. Plot it in Leaflet
```{r}
# create color scale
pal <- colorNumeric("YlOrRd", domain = tile_activity_lonlat$activity, na.color = "transparent")

## create leaflet map
leaflet(tile_activity_lonlat) %>% 
  addTiles() %>% 
  addPolygons(fillColor = ~pal(activity),
              weight = 1, color = "black", fillOpacity = 0.7,
              popup = ~paste0("Activity", round(activity, 3))) %>% 
  addLegend(pal = pal, values = ~activity, title = "Activity Level", position = "bottomright")


```

##Step 4. Summarize and merge with polygon boundary
- your CRS might be different (play around with st_valid() etc)

```{r}
# fix my polygons
casp_shp_clean_meta1 <- casp_shp_clean %>% st_make_valid()
casp_shp_clean_meta <- st_transform(casp_shp_clean_meta1, crs = st_crs(tile_activity_lonlat))

# find intersection bwn meta data & park polygons
activity_casp <- st_intersection(activitymap_us1, casp_shp_clean_meta)

# summarise activity data
activity_casp_summary <- activity_casp %>% 
  group_by(UNITNAME) %>% 
  summarise(mean_activity = mean(activity, na.rm = TRUE),
            max_activity = max(activity, na.rm = TRUE), 
            sd_activity = sd(activity, na.rm = TRUE), .groups = "drop") %>% 
  st_drop_geometry()

# merge data with park data
casp_meta_activity <- casp_shp_clean  %>% 
  left_join(activity_casp_summary, by = "UNITNAME")
## ^^ this is my final dataset that I'm building with other covariates

```
