---
title: "1_DataCleaning_20260129"
output: html_document
date: "2026-01-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## upload necessary packages
library(tidyverse)
library(tigris)
library(sf)
library(osmdata)
library(biscale)
library(terra)
library(janitor)

```

Upload necessary data
```{r}
# ca counties
ca_shp <- counties(state = "CA", cb = TRUE, class = "sf", year = 2022)

# ca state park (casp) info
casp_shp <- st_read("data/raw/parks/casp_individualpark_boundaries/ParkBoundaries.shp")
casp_districtdivision_shp <- st_read("data/raw/parks/casp_district_boundaries/Districts.shp")

# tick info
tick_year_inf <- rast("data/raw/ticks/ipac_year_infection_predictions.tif")
tick_year_presence <- rast("data/raw/ticks/ipac_year_suitability_predictions.tif")
# (tick monthly precense is uploaded below within a function)
```

#Park info

a. Clean up names
```{r}
# fix division names
casp_districtdivision_shp <- casp_districtdivision_shp %>% 
  mutate(field_divi = case_when(field_divi == "Northern Field Division" ~ "Northern Field Division",
                                field_divi == "Nothern Field Division" ~ "Northern Field Division",
                                TRUE ~ field_divi))

# select certain parks which are more likely to have recreation
casp_shp_select <- casp_shp %>% # 462 original parks
  filter(grepl(" SP", UNITNAME) |
           grepl(" SRA", UNITNAME) |
           grepl(" SB", UNITNAME) |
           grepl(" SNR", UNITNAME) |
           grepl(" SW", UNITNAME)) # 254 select parks

## add UNITNAMES to division info

# first correct the crs
casp_shp_select_crs <- st_transform(casp_shp_select, st_crs(casp_districtdivision_shp))  

# if parks overlap multiple districts use st_intersects (rather than st_within)
casp_shp_select_crs <- st_join(casp_shp_select_crs, casp_districtdivision_shp, join = st_intersects) 

# check which parks were assigned NA & fix
casp_shp_select_crs <- casp_shp_select_crs %>% #filter(is.na(field_divi)) 
  mutate(field_divi = case_when(UNITNAME == "Cardiff SB" ~ "Coastal Field Division",
                                UNITNAME == "Manchester SP" ~ "Nothern Field Division",
                                UNITNAME == "Silver Strand SB" ~ "Coastal Field Division",
                                TRUE ~ field_divi),
         district = case_when(UNITNAME == "Cardiff SB" ~ "San Diego Coast District",
                              UNITNAME == "Manchester SP" ~ "Sonoma-Mendocino Coast District",
                              UNITNAME == "Silver Strand SB" ~ "San Diego Coast District",
                              TRUE ~ district)) 

# select subset of columns
casp_shp_clean <- casp_shp_select_crs %>% 
  dplyr::select(UNITNAME, Shape_Area, field_divi, district, geometry) 

```

b. Add distance metadata
```{r}
### Calculate distance from park centroid to coast

## get coastline of ca
ca_osm <- getbb(place_name = "California") %>% 
  opq() %>% # creates overpass API query
  add_osm_feature(key = "natural", value = "coastline") %>% 
  osmdata_sf() # return as sf object


# clean up coastline data
coastlines <- ca_osm$osm_lines %>% 
  st_simplify(dTolerance = 0.01) %>%  # reduce complexity 
  st_union() %>% # merge into cont geometry
  st_cast('LINESTRING') # helps for calculations


## set up parks shp data
casp_shp_sf <- casp_shp %>% 
  st_transform(crs = 4326) %>%
  st_make_valid()

# extract bbox of each feature
casp_shp_bbox <- do.call(rbind, lapply(st_geometry(casp_shp_sf), st_bbox)) %>% 
  as.data.frame()

# combine bbox data with shp data
casp_shp_trans <- bind_cols(casp_shp_sf,casp_shp_bbox)

# calc centroids
casp_shp_cent <- st_centroid(casp_shp_trans)

# to calculate distances convert for meters
casp_shp_cent_proj <- st_transform(casp_shp_cent, crs = 3310)
coastlines_proj <- st_transform(coastlines, crs = 3310)

# calc distance from centroids to coastline
distances <- st_distance(casp_shp_cent_proj, coastlines_proj)

# extract min distance for each feature
min_distance <- apply(distances, 1, min) # in meters

min_distance_km <- min_distance/1000

## adad back to spatial data
casp_shp_sf$dist_coast_km <- min_distance_km

# casp_shp_sf %>% st_drop_geometry() %>% write.csv("data/processed/metadata/casp_dist_coast_km.csv")

```

#CASP visitors
```{r}
casp_visit_raw <- read.csv("data/raw/recreation/casp/Statistical_report_2023_PublicView_-1151597945676077515.csv") %>% clean_names()

# there's multiple years so calculate mean total 
casp_year_total <- casp_visit_raw %>% 
  rename(UNITNAME = park_unit) %>% 
  mutate(UNITNAME = case_when(UNITNAME == "Anza Borrego Desert SP" ~ "Anza-Borrego Desert SP",
                            UNITNAME == "Bidwell Sacramento River SP" ~ "Bidwell-Sacramento River SP",
                            UNITNAME == "Bothe Napa Valley SP" ~ "Bothe-Napa Valley SP",
                            UNITNAME == "Colusa Sacramento River SRA" ~ "Colusa-Sacramento River SRA",
                            UNITNAME == "McArthur Burney Falls Memorial SP" ~ "McArthur-Burney Falls Memorial SP",
                            UNITNAME == "Plumas Eureka SP" ~ "Plumas-Eureka SP",
                            UNITNAME == "Standish Hickey SRA" ~ "Standish-Hickey SRA",
                            UNITNAME == "Tomo Kahni SHP" ~ "Tomo-Kahni SHP",
                            UNITNAME == "Trione Annadel SP" ~ "Trione-Annadel SP",
                            UNITNAME == "Westport Union Landis SB" ~ "Westport-Union Landis SB",
                            TRUE ~ UNITNAME)) %>% 
  group_by(UNITNAME) %>% 
  summarise(casp_year_meantotal = if(n() == 1) total[1] else mean(total, na.rm = TRUE))


#write.csv(casp_year_total, file = "data/processed/casp_year_totalvisitors.csv")

```


#Tick info
- infected: data comes from Andy's 2022 paper
- suitable: habitat suitability from Andy's 2020 paper

a. annual **infected** tick predictions per park
```{r}
## fix park boundaries & match crs of tick data
casp_shp_vect <- casp_shp_clean %>% 
  st_make_valid() %>% 
  st_simplify(dTolerance = 0.001, preserveTopology = TRUE) %>% 
  st_transform(crs = crs(tick_year_inf)) %>% 
  vect()

## extract tick predictions per park
tick_inf_means <- terra::extract(tick_year_inf, casp_shp_vect, fun = mean, na.rm = TRUE)

## turn into dataframe
casp_tick_df <- bind_cols(as.data.frame(casp_shp_vect), tick_inf_means)
names(casp_tick_df)[6] <- "tick_year_inf"

# select distinct parks 
casp_tick_inf_distinct <- casp_tick_df %>% 
  distinct(UNITNAME, .keep_all = TRUE) %>% 
  dplyr::select(UNITNAME, tick_year_inf)

```

b. annual **suitable** tick predictions per park
```{r}
## fix park boundaries and match CRS with presence 
casp_shp_vect_suit <- casp_shp_clean %>% 
  st_make_valid() %>% 
  st_simplify(dTolerance = 0.001, preserveTopology = TRUE) %>% 
  st_transform(crs = crs(tick_year_presence)) %>% 
  vect()

## extract tick predictions per park
tick_presence_means <- terra::extract(tick_year_presence, casp_shp_vect_suit, fun = mean, na.rm = TRUE)

## turn into dataframe
casp_tick_suit_df <- bind_cols(as.data.frame(casp_shp_vect_suit), tick_presence_means)
names(casp_tick_suit_df)[6] <- "tick_year_presence"

# select for distinct parks
casp_tick_suit_distinct <- casp_tick_suit_df %>% 
  distinct(UNITNAME, .keep_all = TRUE) %>% 
  dplyr::select(UNITNAME, tick_year_presence)
```

c. merge annual data
```{r}
tick_park_annual <- casp_tick_inf_distinct %>% 
  left_join(casp_tick_suit_distinct, by = "UNITNAME")

#tick_park_annual %>% write.csv("data/processed/interim_files/tick_park_annual.csv")

```

d. monthly **suitable** tick predictions per park
```{r}

## write function to call in all tiff files
tick_month_tiff <- list.files("data/raw/ticks/ipac_month_suitability_predictions/", full.names = TRUE)
month_names <- month.abb[1:length(tick_month_tiff)]

## fix park crs
sample_rast <- rast(tick_month_tiff[1])

# create another vect just to make sure it matches this rast
casp_shp_vect_suit_monthly <- casp_shp_clean %>% 
  st_make_valid() %>% 
  st_simplify(dTolerance = 0.001, preserveTopology = TRUE) %>% 
  st_transform(crs = crs(sample_rast)) %>% 
  vect()

## loop through all months to extract mean presence suitability per park
casp_tick_monthly_suit <- map2_dfr(.x = tick_month_tiff, .y = month_names,
                                   .f = function(tif, month) {
                                     rast_month <- rast(tif)
                                     
                                     means <- terra::extract(rast_month, casp_shp_vect_suit_monthly, fun = mean, na.rm = TRUE)
                                     
                                     # add park name
                                     df <- as.data.frame(means)
                                     df$UNITNAME <- casp_shp_vect_suit_monthly$UNITNAME
                                     df$month <- month
                                     colnames(df)[2] <- "tick_monthly_suitability"
                                     
                                     return(df[, c("UNITNAME", "month", "tick_monthly_suitability")])
                                   })


casp_tick_monthly_suit$month <- factor(casp_tick_monthly_suit$month, levels = month.abb)

#write.csv(casp_tick_monthly_suit, file = "data/processed/interim_files/casp_tick_monthly_suit_long.csv")

## make wide
casp_tick_monthly_suit_wide <- casp_tick_monthly_suit %>% 
  group_by(UNITNAME, month) %>%
  summarise(tick_monthly_suitability = mean(tick_monthly_suitability, na.rm = TRUE), .groups = "drop") %>% 
  pivot_wider(names_from = month,
              values_from = tick_monthly_suitability,
              names_prefix = "tick_month_presence_")
```

e. merge all annual and monthly tick predictions per park
```{r}

tick_park_annualmonthly <- tick_park_annual %>% 
  left_join(casp_tick_monthly_suit_wide, by = "UNITNAME") %>% 
  dplyr::select(UNITNAME:tick_month_presence_Dec) 


#tick_park_annualmonthly %>% write.csv("data/processed/tick_casp_allextractions.csv")

```

tick inf at zipcode
```{r}
ldi <- st_read("data/raw/LD_incidence/CA_Zips_Lyme_NAD83.shp")
hist(ldi$LDI_Cat)

## format spatial data
#correct CRS
ldi_crs <- st_transform(ldi, crs = 4326)
# vectorize ldi
ldi_vect <- vect(ldi_crs)



## fix park boundaries & match crs of tick data
zip_shp_vect <- ldi_crs %>% 
  st_make_valid() %>% 
  st_simplify(dTolerance = 0.001, preserveTopology = TRUE) %>% 
  st_transform(crs = crs(tick_year_inf)) %>% 
  vect()


## extract tick predictions per zip
tick_inf_means_zip <- terra::extract(tick_year_inf, zip_shp_vect, fun = mean, na.rm = TRUE)

## turn into dataframe
zip_tick_df <- bind_cols(as.data.frame(zip_shp_vect), tick_inf_means_zip)
names(zip_tick_df)[168] <- "tick_year_inf"

# select distinct zip
zip_tick_inf_distinct <- zip_tick_df %>% 
  distinct(ZIP_CODE, .keep_all = TRUE) %>% 
  dplyr::select(ZIP_CODE, tick_year_inf) %>% 
  rename(zip_code = ZIP_CODE)
  
write.csv(zip_tick_inf_distinct, file = "data/processed/zip_tick_inf_distinct.csv", row.names = FALSE)

```

#LDI at zip code & within state parks
```{r}
# Zip code level LDI was calculated by Eisen et al 2006b as the mean annual incidence of endemic LD for the period 1993–2005, dropping cases within that date range where the likely county of exposure did not match county of residence, or for which residential zip code information was missing. Eisen et al reported LDI by zip code in the following categories: 0, 0.01–1, 1.01–5, 5.01–10, 10.01–20, 20.01–50, and >50 per 100.000 population

ldi <- st_read("data/raw/LD_incidence/CA_Zips_Lyme_NAD83.shp")
hist(ldi$LDI_Cat)

## format spatial data
#correct CRS
ldi_crs <- st_transform(ldi, crs = 4326)
# vectorize ldi
ldi_vect <- vect(ldi_crs)
# i think ill format this data to WGS84/4326 for later visualizations
casp_shp_vect_ldi <- casp_shp_clean %>% 
  st_make_valid() %>% 
  st_simplify(dTolerance = 0.001, preserveTopology = TRUE) %>% 
  st_transform(crs = crs(ldi_crs)) %>% 
  vect()


## calculate area-weighted average (if park overlaps multiple ZIP codes, compute LDI for the park as area-weighted average of ZIP-level LDI values intersecting it)
# give more weight to ZIP codes that cover more area within the park


# which parks intersect with LDI zipcodes
intersections <- terra::intersect(casp_shp_vect_ldi, ldi_vect)

# calculate area of intersected pieces
intersections$area <- terra::expanse(intersections, unit = "km")

# compute area-weighted LDI per park
intersections$LDI_cat <- as.numeric(as.character(intersections$LDI_Cat)) # make sure numeric

# aggreagate by park
ldi_year_weightedavg <- intersections %>% 
  as.data.frame() %>% 
  group_by(UNITNAME) %>% 
  summarise(area_weighted_ldi = weighted.mean(LDI_Cat, w = area, na.rm = TRUE)) %>% 
  mutate(area_weighted_ldi = round(area_weighted_ldi,3)) %>% 
  rename(ldi_year_weightedavg_casp = area_weighted_ldi)

```

#make annual datasets

at the casp level
```{r}
## slim down the distance for joining
dist_slim <- read.csv("data/processed/casp_dist_coast_km.csv") %>%   # 462 original parks
          filter(grepl(" SP", UNITNAME) |
           grepl(" SRA", UNITNAME) |
           grepl(" SB", UNITNAME) |
           grepl(" SNR", UNITNAME) |
           grepl(" SW", UNITNAME)) %>% 
  distinct(UNITNAME, .keep_all = TRUE) %>% 
  dplyr::select(UNITNAME, dist_coast_km)


casp_ldi_tick_annualmonthly <- 
casp_shp_clean %>% st_drop_geometry() %>% distinct(UNITNAME, .keep_all = TRUE) %>% dplyr::select(-Shape_Area) %>% 
  left_join(dist_slim , by = "UNITNAME") %>% 
  left_join(ldi_year_weightedavg, by = "UNITNAME") %>% 
  left_join(tick_park_annualmonthly, by = "UNITNAME")

#casp_ldi_tick_annualmonthly %>% write.csv("data/processed/casp_ldi_tick_annualmonthly.csv")

```